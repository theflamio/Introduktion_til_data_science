---
title: "Data Science Exam: Part 2"
author: "Flemming Christensen"
date: "2024-10-6"
format:
  html:
    embed-resources: true
---

# Libraries
```{r, results="hide"}
library(doBy)
library(tidyverse)
library(ggplot2)
library(reshape2)
library(rpart)
library(rpart.plot)
library(randomForest)
library(readr)
library(dplyr)
library(tidyr)
library(MASS)
library(caret)
```


# Part 1: Regression

# Data
This part uses the `personality` dataset from the R package `doBy` with recordings of 32 variables describing personality characteristics for 240 people. (see help file for `?personality`)

We focus on the variable `easygon` as the response variable. We split the data in training and test data as follows:

```{r}
dat <- doBy::personality
set.seed(101) # for reproducibility
i_train <- sample(nrow(dat), .5*nrow(dat))
train <- dat[i_train,]
test <- dat[-i_train,]
```


# Exercise 1
Use `response_plot()` from the **development version of** `doBy` to visualize the relation between the response and the other variables in the data.

```{r, fig.width=10, fig.height=20}
response_plot(dat, easygon ~ ., geoms = c(geom_jitter(alpha = .6, width = .4, height = .4)), ncol = 5)
```


# Exercise 2
Specify a number of prediction models â€“ at least one of each of the following:

1. linear with stepwise selection (`lm`/ `step`)
2. regression tree (`rpart`)
3. random forest (`randomForest`)

## General observations
predictors that seems to be significant predictors from the response plot are following.

Very strong: relaxed - laidbck.
strong: coopera - approvn.

Lets see if that holds up later on.

## 1. linear with stepwise selection (`lm`/ `step`)

### 1.1 Fit a linear model with all variabels
```{r, results="hide"}
# Fit a linear model with all variabels
lm_full <- lm(easygon ~ ., data = train)
```

### 1.2 stepwise selection forward_model
`Null_model` purpose in `forward stepwise selection` is to give a starting point with no predictors.
allows the algorithm to add variables incrementally.

```{r, results="hide"}
null_model <- lm(easygon ~ 1, data = train)

lm_forward <- step(null_model, direction = "forward", scope = formula(lm_full), trace = FALSE)
lm_forward_aic <- AIC(lm_forward)
```

### 1.3 stepwise selection backward_model
Starts with a full model then eliminate least significant predictor

```{r, results="hide"}
lm_backward <- step(lm_full, direction = "backward", trace = FALSE)
lm_backward_aic <- AIC(lm_backward)
```

### 1.4 stepwise selection both_model
Starts with a full model that allow for both additions and removals.
Since it goes back and forward we will use the null model.
```{r, results="hide"}
lm_both <- step(null_model, direction = "both", scope = formula(lm_full), trace = FALSE)

lm_both_aic <- AIC(lm_both)
```

### Compare AIC values 
Lower AIC values are preferred, as they indicate a better balance between model fit and complexity

```{r}
aic_values <- data.frame(
  Method = c("lm_Forward", "lm_Backward", "lm_Both"),
  AIC = c(lm_forward_aic, lm_backward_aic, lm_both_aic)
)

print(aic_values)
```


## 2. regression tree (`rpart`)

### 2.1 regression tree.

```{r, results="hide"}
set.seed(101) # for reproducibility

# Fit a regression tree model
tree_model <- rpart(easygon ~ ., data = train, cp = 0)
```

### 2.2 complexity parameter table.

```{r}
printcp(tree_model)
# CP:       "Complexity parameter" penalty for adding split. lower values more complex trees
# nsplit:   "Number of splits in the tree"
# rel error "relative error of the model compare to the root node. lower values are better"
# xerror:   "Cross-validated error"
# xstd:     "The standard error of the cross-validated error, providing a measure of variability"
```

### 2.3 Plot the complexity parameter vs. cross-validated error.
X-cal Relative Error = cross-validated relative error

```{r, fig.width=7, fig.height=7}
plotcp(tree_model)
```

### 2.4 pick the optimal cp from previous plot and prune the tree.

```{r, results="hide"}
optimal_cp <- 0.057 ## from printcp/plotcp output. Closset to the mean

pruned_tree_model <- prune(tree_model, cp = optimal_cp)
```

### 2.5 plot of tree model and pruned tree model.

```{r, fig.width=7, fig.height=7}
par(mfrow = c(2,1))
rpart.plot(tree_model, main = "Full tree")
rpart.plot(pruned_tree_model, main = paste("Pruned tree, cp = ", optimal_cp))

```

## 3. random forest (`randomForest`)

### 3.1 random forest

```{r, fig.width=7, fig.height=7}
set.seed(101) # for reproducibility

# Fit a random forest model
rf_default <- randomForest(easygon ~ ., data = train, importance = TRUE)

# Print model details
rf_default

# Plots the importance of each variable, helping identify which predictors are most influential. "Only for random forest"
varImpPlot(rf_default)
```
### 3.2 Tuning random forest
By fixing mtry with ntree we tune the random forest

```{r}
set.seed(101) 
rf_tuned <- tuneRF(train[, -which(names(train) == "easygon")],  # Exclude target variable
                    train$easygon,                             # Target variable
                    ntreeTry = 200,                           # Number of trees to try
                    stepFactor = 1.5,                        # Factor by which to increase mtry
                    improve = 0.01,                          # Minimum improvement in OOB error
                    trace = TRUE,                             # Print progress
                    plot = TRUE)

best_mtry_value <- rf_tuned[which.min(rf_tuned[, "OOBError"]), "mtry"] # select best model

rf_final <- randomForest(easygon ~ ., 
                         data = train, 
                         mtry = 7, 
                         ntree = 200,        
                         importance = TRUE)
```


# Exercise 3
For **one of** the linear models show the estimated coefficients and comment **briefly** on the output (for example, comment on the sign of the estimates).

```{r}
broom::tidy(lm_forward)
```
term:       Predictor variable in the model.

estimate:   if the estimate of x is 2 the dependent variable "y" will increase by 2 for every x.

std.error:  Small standard error indicates that the estimate is precise.

statistic:  Also known as T statistic.

p.value:    A small p-value (typically < 0.05) suggests that there is strong evidence against the null hypothesis,
            leading to its rejection, which implies that the term is statistically significant in predicting the                  dependent variable.

# Exercise 4
Compare the performance of the fitted models by predicting the values of `easygon` for the test data and show a plot comparing the predictions to the observations from the test data. Also make a table of the RMSE for each model.

```{r}
# Predict on test data
pred_lm_full <- predict(lm_full, newdata = test)
pred_lm_forward <- predict(lm_forward, newdata = test)
pred_lm_backward <- predict(lm_backward, newdata = test)
pred_lm_both <- predict(lm_both, newdata = test)
pred_rpart <- predict(tree_model, newdata = test)
pred_rpart_prune <- predict(pruned_tree_model, newdata = test)
pred_rf <- predict(rf_default, newdata = test)
pred_rf_tuned <- predict(rf_final, newdata = test)

# Extract actual values from the test set
actual <- test$easygon

# Function to calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Calculate RMSE for each model
rmse_lm_full <- rmse(actual, pred_lm_full)
rmse_lm_forward <- rmse(actual, pred_lm_forward)
rmse_lm_backward <- rmse(actual, pred_lm_backward)
rmse_lm_both <- rmse(actual, pred_lm_both)
rmse_rpart <- rmse(actual, pred_rpart)
rmse_rpart_prune <- rmse(actual, pred_rpart_prune)
rmse_rf <- rmse(actual, pred_rf)
rmse_rf_tuned <- rmse(actual, pred_rf_tuned)

# Create a table to compare RMSE
rmse_table <- data.frame(
  Model = c("lm_full", "lm_forward", "lm_backward", "lm_both", "Tree full", "Tree_pruned", "RF", "RF_tuned"),
  RMSE = c(rmse_lm_full, rmse_lm_forward, rmse_lm_backward, rmse_lm_both, rmse_rpart, rmse_rpart_prune, rmse_rf, rmse_rf_tuned)
)

# Print RMSE table
print(rmse_table)
```

```{r}
# Combine the actual and predicted values into a single data frame for plotting
plot_data <- data.frame(
  Observed = actual,
  lm_full = pred_lm_full,
  lm_forward = pred_lm_forward,
  lm_backward = pred_lm_backward,
  lm_both = pred_lm_both,
  Tree = pred_rpart,
  Tree_pruned = pred_rpart_prune,
  RF = pred_rf,
  RF_tuned = pred_rf_tuned
)

# Melt the data frame to long format for ggplot
plot_data_melt <- melt(plot_data, id.vars = "Observed", 
                       variable.name = "Model", value.name = "Predicted")

# Create the plot
ggplot(plot_data_melt, aes(x = Observed, y = Predicted, color = Model)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = "Observed vs Predicted Values of easygon",
       x = "Observed easygon", y = "Predicted easygon") +
  facet_wrap(~Model)
```


# Part 2: Classification
The dataset `diabetes.csv` is about risk factors for diabetes and contains 768 observations of 9 variables. The response variable is `Outcome` which is binary. The goal is to predict `Outcome` using the other variables. A description of the dataset can be found at https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data

# Exercise 1
Read in and clean the data by detecting unexpected values of some variables, replace the values with `NA` and finally make a data set with only the complete cases.

## 1.1 Quick look at the data

```{r}
data_dir <- here::here("diabetes.csv") #set the directory to were the project and dataset is used

dia_raw_dat = suppressMessages(read_csv(data_dir)) # Read csv file

head(dia_raw_dat) #check for correct data type "obs outcome is not a factor"

summary(dia_raw_dat) #quick and dirty look at the raw data alot of zeroes !
```

## Diabetes information and dataset observations

1. Pregnancies: Number of times pregnant "Obs missing genders male/female"

2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test

3. BloodPressure: Diastolic blood pressure (mm Hg)

4. SkinThickness: Triceps skin fold thickness (mm)

5. Insulin: 2-Hour serum insulin (mu U/ml)

6. BMI: Body mass index (weight in kg/(height in m)^2)

7. DiabetesPedigreeFunction aka genetic risk factor: DPF of 0.5 = moderate risk and DPF of 0.9 high genetic risk factor.

8. Age: Age (years)

9. Outcome: Class variable (0 or 1)

zero value in any of the variable beside Pregnancy and Outcome is unrealistic

Seems like the data set have used 0 and 0.0 to describe missing values. Needs to be changed to NA

## 1.2 Clean the data set

```{r}
dia_dat_with_NA <- dia_raw_dat |>
  mutate(	
    Glucose = ifelse(Glucose == 0, NA, Glucose),
    BloodPressure = ifelse(BloodPressure == 0, NA, BloodPressure),
    SkinThickness = ifelse(SkinThickness == 0, NA, SkinThickness),
    Insulin = ifelse(Insulin == 0, NA, Insulin),
    BMI = ifelse(BMI == 0, NA, BMI),
    DiabetesPedigreeFunction = ifelse(DiabetesPedigreeFunction == 0, NA, DiabetesPedigreeFunction)
  )

dia_clean_dat <- na.omit(dia_dat_with_NA) # only the complete cases

head(dia_clean_dat)

summary(dia_clean_dat) #quick and dirty look at the clean data
```


# Exercise 2
Plot the response against each of the explanatory variables/features and add a smooth line as a very rough indication of how the probability of diabetes depends on the individual feature (note the response has to be numeric for the smooth line to work).

## 2.1 Response plot

```{r, fig.width=7, fig.height=15}

# Convert data to long format to use facetting
dia_long <- dia_clean_dat |>
  pivot_longer(cols = c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age"),
               names_to = "Feature",
               values_to = "Value")

dia_long

ggplot(dia_long, aes(x = Value, y = Outcome)) +
  geom_point(position = position_jitter(width = 0, height = 0.1),alpha = 0.5) +  # Scatterplot
  geom_smooth(method = "loess", color = "blue", se = TRUE) +  # Smooth line with confidence interval
  facet_wrap(~ Feature, scales = "free_x") +  # Facet by Feature, adjust scales
  labs(
    title = "Outcome vs Features",
    x = "Feature Value",
    y = "Outcome aka Probability of Diabetes"
  )
```


# Exercise 3
Ensure Outcome is a factor and split the data in a training and test set as follows:

## 3.1

```{r}
#convert Outcome to factor !
dia_clean_dat$Outcome <- factor(dia_clean_dat$Outcome, levels = c(0, 1), labels = c("No", "Yes"))

head(dia_clean_dat)

set.seed(202) # for reproducibility
i_train <- sample(nrow(dia_clean_dat), .5*nrow(dia_clean_dat))
train <- dia_clean_dat[i_train,]
test <- dia_clean_dat[-i_train,]
```

Specify a number of classification models and report the corresponding confusion matrix for the test data. You must include at least one of each of the following:

1. logistic regression with stepwise selection (`glm` / `step`)
2. regression tree (`rpart`)
3. random forest (`randomForest`)
4. linear/quadratic discriminant analysis

## 3.2 logistic regression with stepwise selection (`glm` / `step`)

### 3.2.1 fit log model with stepwise forward selection
```{r, results="hide"}
set.seed(202)
log_model <- glm(Outcome ~., data = train, family = "binomial")

summary(log_model)

# from the plot it suggest that Glucose, DiabetesPedigreeFunction, BMI and Blood pressure are important.
log_model_refined <- glm(Outcome ~ Glucose + DiabetesPedigreeFunction + BMI +BloodPressure, data = train, family = "binomial")

summary(log_model_refined)

# Stepwise forward selection
log_model_stepwise <- step(null_model, 
                           direction = "forward", 
                           scope = formula(log_model),
                           trace = FALSE)

summary(log_model_stepwise)
```

### 3.2.2 Make prediction using test set with the stepwise selected model and log model

```{r}
predicted_prob_log <- predict(log_model, newdata = test, type = "response")
predicted_class_log <- ifelse(predicted_prob_log > 0.5, "Yes", "No")
predicted_class_log <- factor(predicted_class_log, levels = c("No", "Yes"))

predicted_prob_refine <- predict(log_model_refined, newdata = test, type = "response")
predicted_class_refine <- ifelse(predicted_prob_refine > 0.5, "Yes", "No")
predicted_class_refine <- factor(predicted_class_refine, levels = c("No", "Yes"))

predicted_prob_stepwise <- predict(log_model_stepwise, newdata = test, type = "response")
predicted_class_stepwise <- ifelse(predicted_prob_stepwise > 0.5, "Yes", "No")
predicted_class_stepwise <- factor(predicted_class_stepwise, levels = c("No", "Yes"))

```

### 3.2.3 Evaluate the log model and stepwise selected model

```{r}
confusion_matrix <- confusionMatrix(predicted_class_log, test$Outcome, positive = "Yes")
print("Confusion Matrix table log:")
print(confusion_matrix$table)
print(confusion_matrix$overall['Accuracy'])

confusion_matrix <- confusionMatrix(predicted_class_refine, test$Outcome, positive = "Yes")
print("Confusion Matrix table log refine:")
print(confusion_matrix$table)
print(confusion_matrix$overall['Accuracy'])

confusion_matrix <- confusionMatrix(predicted_class_stepwise, test$Outcome, positive = "Yes")
print("Confusion Matrix table log stepwise:")
print(confusion_matrix$table)
print(confusion_matrix$overall['Accuracy'])
```


## 3.3 regression tree (`rpart`)

```{r, fig.width=15, fig.height=7}
set.seed(202)

tree_model <- rpart(Outcome ~ ., data = train, method = "anova")

# Visualize the tree
rpart.plot(tree_model, main = "Full tree")
```

## 3.4 random forest (`randomForest`)

```{r, results="hide"}
set.seed(202)

# Fit a random forest model
rf_model <- randomForest(Outcome ~ ., data = train, importance = TRUE)

# Print model details
print(rf_model)

# Visualizes how the model's error rate changes with the number of trees.
plot(rf_model, main = "Random Forest Error Rates")

# Plots the importance of each variable, helping identify which predictors are most influential. "Only for random forrest"
varImpPlot(rf_model)
```

## 4. linear/quadratic discriminant analysis

### 4.1 linear discriminant analysis
```{r, results="hide"}
set.seed(202)
lda_model <- lda(Outcome ~ ., data = train)

lda_predictions <- predict(lda_model)

head(lda_predictions$class)
```

### 4.2 quadratic discriminant analysis

```{r}
qda_model <- qda(Outcome ~ ., data = train)

qda_predictions <- predict(qda_model)

head(qda_predictions$class)
```


## Project dependencies

```{r}
sessionInfo()
```