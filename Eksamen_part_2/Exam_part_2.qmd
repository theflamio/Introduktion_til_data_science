---
title: "Data Science Exam: Part 2"
author: "Flemming Christensen"
date: "2024-10-14"
format:
  html:
    toc: true  # Enable the table of contents
    toc_depth: 3  # Specify the depth of headers to include in the TOC
    embed-resources: true
---

# Libraries
```{r, results="hide"}
library(doBy)
library(tidyverse)
library(ggplot2)
library(ggcorrplot)
library(reshape2)
library(rpart)
library(rpart.plot)
library(randomForest)
library(readr)
library(dplyr)
library(tidyr)
library(MASS)
library(caret)
```


# Part 1: Regression

# Data
This part uses the `personality` dataset from the R package `doBy` with recordings of 32 variables describing personality characteristics for 240 people. (see help file for `?personality`)

We focus on the variable `easygon` as the response variable. We split the data in training and test data as follows:

```{r}
dat <- doBy::personality
set.seed(101) # for reproducibility
i_train <- sample(nrow(dat), .5*nrow(dat))
train <- dat[i_train,]
test <- dat[-i_train,]
```


# Exercise 1
Use `response_plot()` from the **development version of** `doBy` to visualize the relation between the response and the other variables in the data.

## Plot the relation between the repsone and the other variables.

```{r, fig.width=10, fig.height=20}
response_plot(dat, easygon ~ ., geoms = c(geom_jitter(alpha = .6, width = .4, height = .4)), ncol = 5)
```


# Exercise 2
Specify a number of prediction models â€“ at least one of each of the following:

1. linear with stepwise selection (`lm`/ `step`)
2. regression tree (`rpart`)
3. random forest (`randomForest`)

## General observations
predictors that seems to be significant predictors from the response plot are following.

Very strong: relaxed - laidbck.
strong: coopera - approvn.

Lets see if that holds up later on.

## 1. Linear with stepwise selection (`lm`/ `step`)

### 1.1 Fit a linear model with all variabels
```{r, results="hide"}
# Fit a linear model with all variabels
lm_full <- lm(easygon ~ ., data = train)
```

### 1.2 Stepwise selection forward_model
`Null_model` purpose in `forward stepwise selection` is to give a starting point with no predictors.
allows the algorithm to add variables incrementally.

```{r, results="hide"}
null_model <- lm(easygon ~ 1, data = train)

lm_forward <- step(null_model, direction = "forward", scope = formula(lm_full), trace = FALSE)
lm_forward_aic <- AIC(lm_forward)
```

### 1.3 Stepwise selection backward_model
Starts with a full model then eliminate least significant predictor

```{r, results="hide"}
lm_backward <- step(lm_full, direction = "backward", trace = FALSE)
lm_backward_aic <- AIC(lm_backward)
```

### 1.4 Stepwise selection both_model
Starts with a full model that allow for both additions and removals.
Since it goes back and forward we will use the null model.
```{r, results="hide"}
lm_both <- step(null_model, direction = "both", scope = formula(lm_full), trace = FALSE)

lm_both_aic <- AIC(lm_both)
```

### Compare AIC values 
Lower AIC values are preferred, as they indicate a better balance between model fit and complexity

```{r}
aic_values <- data.frame(
  Method = c("lm_Forward", "lm_Backward", "lm_Both"),
  AIC = c(lm_forward_aic, lm_backward_aic, lm_both_aic)
)

print(aic_values) # smallest best
```


## 2. Regression tree (`rpart`)

### 2.1 Regression tree.

```{r, results="hide"}
set.seed(101) # for reproducibility

# Fit a regression tree model
tree_model <- rpart(easygon ~ ., data = train)
```

### 2.2 Complexity parameter table.

```{r}
cp_table = printcp(tree_model)
# CP: The complexity parameter value.
# nsplit: The number of splits in the tree corresponding to each CP.
# rel error: The relative error of the model compared to the root node.
# xerror: Cross-validated error, a key metric for assessing model performance.
# xstd: Standard error of the cross-validated error.
```

### 2.3 Plot the complexity parameter vs. cross-validated error.
X-cal Relative Error = cross-validated relative error

```{r, fig.width=7, fig.height=7}
plotcp(tree_model)

# X-axis: Represents the complexity parameter (CP). Lower CP values lead to larger, more complex trees.

# Y-axis: Shows the cross-validated error (xerror). Lower values indicate better predictive performance.
```

### 2.4 Pick the optimal cp from printcp and previous plot. Then prune the tree.
from printcp cp =  0.070189 as the optimal cp because it has the lowest cross-validation error. more accurate model and simple model 1 split.
Offers a balance of simplicity and predictive performance.

from printcp cp = 0.046342, as it is within one standard error of the minimum xerror "0.86815" "1-SE Rule". more complex model.
Provides improved flexibility and model fit.

from plotcp output cp = 0.12 is the best performing but also the most complex.

from printcp()
1-SE Rule Threshold = Minimum xerror + Standard error = 0.86815 + 0.13437 = 1.00252
(cp = 0.046342 xerror = 0.89836) -> 0.89836 < 1.00252 = True.

```{r, results="hide"}
optimal_cp <- 0.046342 

pruned_tree_model <- prune(tree_model, cp = optimal_cp)
```

### 2.5 Visualize the tree and pruned tree

```{r, fig.width=15, fig.height=7}
par(mfrow = c(2,1))
rpart.plot(tree_model, fallen.leaves = TRUE, main = "Tree model")
rpart.plot(pruned_tree_model, fallen.leaves = TRUE, main = "Pruned Tree model")
```
## 3. Random forest (`randomForest`)

### 3.1 Random forest

```{r, fig.width=7, fig.height=7}
set.seed(101) # for reproducibility

# Fit a random forest model
rf_default <- randomForest(easygon ~ ., data = train, importance = TRUE)
```

### 3.2 Tuning random forest
By changing mtry with fixed ntree we tune the random forest

```{r}
set.seed(101) 
rf_tuned <- tuneRF(train[, -which(names(train) == "easygon")],  # Exclude target variable
                    train$easygon,                              # Target variable
                    ntreeTry = 500,                             # Number of trees to try "default 500"
                    stepFactor = 1,                             # Factor to increase mtry
                    improve = 0.01,                             # Minimum improvement in OOB error
                    trace = FALSE,                              # Print progress
                    plot = FALSE)

best_mtry_value <- rf_tuned[which.min(rf_tuned[, "OOBError"]), "mtry"] # select best model

rf_final <- randomForest(easygon ~ ., 
                         data = train, 
                         mtry = best_mtry_value,        
                         importance = TRUE)
```


# Exercise 3
For **one of** the linear models show the estimated coefficients and comment **briefly** on the output (for example, comment on the sign of the estimates).

```{r}
broom::tidy(lm_forward)
```
term:       Predictor variable in the model.

estimate:   if the estimate of x is 2 the dependent variable "y" will increase by 2 for every x.

std.error:  Small standard error indicates that the estimate is precise.

statistic:  (close to zero) indicating weak evidence of a relationship between the predictor and the dependent variable. 

p.value:    A small p-value (typically < 0.05) suggests that there is strong evidence against the null hypothesis,
            leading to its rejection, which implies that the term is statistically significant in predicting the dependent variable.

## 3.1 briefly

laidback, cooperative, talkative, and giving up have significant positive relationships with the dependent variable, while perseverance and harshness appear to have negative associations. look at estimate and p value.        


# Exercise 4
Compare the performance of the fitted models by predicting the values of `easygon` for the test data and show a plot comparing the predictions to the observations from the test data. Also make a table of the RMSE for each model.

## 4.1 Table of the RMSE for each model

```{r}
# List of models to predict
models <- list(
  lm_full = lm_full,
  lm_forward = lm_forward,
  lm_backward = lm_backward,
  lm_both = lm_both,
  tree_full = tree_model,
  tree_prune = pruned_tree_model,
  rf = rf_default,
  rf_tuned = rf_final
)

# Predict on test data for each model and store predictions in a list
predictions <- lapply(models, predict, newdata = test)

# Extract actual values from the test set
actual <- test$easygon

# Function to calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Calculate RMSE for each model and store results in a vector
rmse_values <- sapply(predictions, rmse, actual = actual)

# Create a table to compare RMSE
rmse_table <- data.frame(
  RMSE = rmse_values
)

# Print RMSE table
print(rmse_table)
```

## 4.2 Plot comparing the predictions to the observations

```{r}
# Combine the actual and predicted values into a single data frame for plotting
plot_data <- data.frame(
  Observed = actual,
  lm_full = predictions$lm_full,
  lm_forward = predictions$lm_forward,
  lm_backward = predictions$lm_backward,
  lm_both = predictions$lm_both,
  tree_full = predictions$tree_full,
  tree_pruned = predictions$tree_prune,
  rf = predictions$rf,
  rf_tuned = predictions$rf_tuned
)

# pivot the data frame to long format for ggplot
plot_data_longer <- plot_data |> 
  pivot_longer(cols = -Observed, 
               names_to = "Model", 
               values_to = "Predicted")

# Create the plot
ggplot(plot_data_longer, aes(x = Observed, y = Predicted, color = Model)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "solid", color = "black") +
  labs(title = "Observed vs Predicted Values of easygon",
       x = "Observed easygon", y = "Predicted easygon") +
  facet_wrap(~Model)
```
# Part 2: Classification
The dataset `diabetes.csv` is about risk factors for diabetes and contains 768 observations of 9 variables. The response variable is `Outcome` which is binary. The goal is to predict `Outcome` using the other variables. A description of the dataset can be found at [Diabetes Data Set on Kaggle](https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data)

# Exercise 1
Read in and clean the data by detecting unexpected values of some variables, replace the values with `NA` and finally make a data set with only the complete cases.

## 1.1 Quick look at the data

```{r}
data_dir <- here::here("diabetes.csv") #set the directory to were the project and dataset is used

dia_raw_dat = suppressMessages(read_csv(data_dir)) # Read csv file

summary(dia_raw_dat) #quick and dirty look at the raw data alot of zeroes !
```

## Diabetes information and dataset observations

1. Pregnancies: Number of times pregnant "Obs missing genders male/female"

2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test

3. BloodPressure: Diastolic blood pressure (mm Hg)

4. SkinThickness: Triceps skin fold thickness (mm)

5. Insulin: 2-Hour serum insulin (mu U/ml)

6. BMI: Body mass index (weight in kg/(height in m)^2)

7. DiabetesPedigreeFunction aka genetic risk factor: DPF of 0.5 = moderate risk and DPF of 0.9 high genetic risk factor.

8. Age: Age (years)

9. Outcome: Class variable (0 or 1)

zero value in any of the variable beside Pregnancy and Outcome is unrealistic

Seems like the data set have used `0` and `0.0` to describe `missing values`. Needs to be changed to `NA`

## 1.2 Clean the data set

```{r}
dia_dat_with_NA <- dia_raw_dat |>
  mutate(	
    Glucose = ifelse(Glucose == 0, NA, Glucose),
    BloodPressure = ifelse(BloodPressure == 0, NA, BloodPressure),
    SkinThickness = ifelse(SkinThickness == 0, NA, SkinThickness),
    Insulin = ifelse(Insulin == 0, NA, Insulin),
    BMI = ifelse(BMI == 0, NA, BMI),
    DiabetesPedigreeFunction = ifelse(DiabetesPedigreeFunction == 0, NA, DiabetesPedigreeFunction)
  )

dia_clean_dat <- na.omit(dia_dat_with_NA) # only the complete cases

summary(dia_clean_dat) #quick and dirty look at the clean data

dim(dia_clean_dat) # dimension of the dataset 392 observation 9 variables
```


# Exercise 2
Plot the response against each of the explanatory variables/features and add a smooth line as a very rough indication of how the probability of diabetes depends on the individual feature (note the response has to be numeric for the smooth line to work).

## 2.1 Response plot

```{r, fig.width=10, fig.height=15}

# Convert data to long format to use facetting
dia_long <- dia_clean_dat |>
  pivot_longer(cols = c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age"),
               names_to = "Feature",
               values_to = "Value")

ggplot(dia_long, aes(x = Value, y = Outcome)) +
  geom_point(position = position_jitter(width = 0, height = 0.1),alpha = 0.5) +  # Scatterplot
  geom_smooth(method = "loess", color = "blue", se = TRUE) +  # Smooth line with confidence interval
  facet_wrap(~ Feature, scales = "free_x") +  # Facet by Feature, adjust scales
  labs(
    title = "Outcome vs Features",
    x = "Feature Value",
    y = "Outcome aka Probability of Diabetes"
  )
```

## 2.2 Correlation plot (bonus)

```{r}
corr_matrix <- cor(dia_clean_dat)

ggcorrplot(corr_matrix, 
           lab = TRUE,                          # Annotate the correlation coefficients
           colors = c("blue", "white", "red"),  # Color scheme similar to 'coolwarm'
           title = "Correlation Graph") +
  theme(plot.title = element_text(size = 11)) + # Set title font size
  theme(aspect.ratio = 6/8)                     # Set figure size (8x6)
```


# Exercise 3
Ensure Outcome is a factor and split the data in a training and test set as follows:

## 3.1 Convert Outcome to factor and split data in train/test set

```{r}
dia_clean_dat$Outcome <- factor(dia_clean_dat$Outcome, levels = c(0, 1), labels = c("No", "Yes"))

head(dia_clean_dat)

set.seed(202) # for reproducibility
i_train <- sample(nrow(dia_clean_dat), .5*nrow(dia_clean_dat))
train <- dia_clean_dat[i_train,]
test <- dia_clean_dat[-i_train,]
```

Specify a number of classification models and report the corresponding confusion matrix for the test data. You must include at least one of each of the following:

1. logistic regression with stepwise selection (`glm` / `step`)
2. regression tree (`rpart`)
3. random forest (`randomForest`)
4. linear/quadratic discriminant analysis

## 3.2 Logistic regression with stepwise selection (`glm` / `step`)

### 3.2.1 Fit log model with stepwise forward selection
```{r, results="hide"}
set.seed(202)
log_model <- glm(Outcome ~., data = train, family = "binomial")

# from the response and correlation plot it suggest that Glucose, DiabetesPedigreeFunction, BMI, Pregnancies and Age are important.
log_model_refined <- glm(Outcome ~ Glucose + DiabetesPedigreeFunction + Pregnancies + BMI + Age, data = train, family = "binomial")

null_model <- glm(Outcome ~ 1, data = train, family = binomial)

# Stepwise forward selection
log_model_stepwise <- step(null_model, 
                           direction = "both", 
                           scope = formula(log_model),
                           trace = FALSE)
```

### 3.2.2 Predict log model, log stepwise selected model and log model and setup confusion Matrix.

```{r}
predicted_prob_log <- predict(log_model, newdata = test, type = "response")
predicted_class_log <- ifelse(predicted_prob_log > 0.5, "Yes", "No")
predicted_class_log <- factor(predicted_class_log, levels = c("No", "Yes"))

confMat_log <- confusionMatrix(predicted_class_log, test$Outcome, positive = "Yes")

predicted_prob_refine <- predict(log_model_refined, newdata = test, type = "response")
predicted_class_refine <- ifelse(predicted_prob_refine > 0.5, "Yes", "No")
predicted_class_refine <- factor(predicted_class_refine, levels = c("No", "Yes"))

confMat_log_refine <- confusionMatrix(predicted_class_refine, test$Outcome, positive = "Yes")

predicted_prob_stepwise <- predict(log_model_stepwise, newdata = test, type = "response")
predicted_class_stepwise <- ifelse(predicted_prob_stepwise > 0.5, "Yes", "No")
predicted_class_stepwise <- factor(predicted_class_stepwise, levels = c("No", "Yes"))

confMat_log_step <- confusionMatrix(predicted_class_stepwise, test$Outcome, positive = "Yes")
```

## 3.3 Regression tree (`rpart`)

### 3.3.1 Fit the Classification tree
```{r}
set.seed(202)

Classification_tree <- rpart(Outcome ~ ., data = train, method = "class")
```

### 3.3.2 Prune the tree "select cp value"
```{r}
printcp(Classification_tree)

plotcp(Classification_tree)
```

### 3.3.3 Prune the tree

```{r}
optimal_cp = 0.048 #lets try it because we want to see how it dose hold up
  
prunedTree <- prune(Classification_tree, cp = optimal_cp)
```
### 3.3.4 Visualize the tree and pruned tree

```{r, fig.width=15, fig.height=7}
par(mfrow = c(2,1))
rpart.plot(Classification_tree, fallen.leaves = TRUE, main = "Classification_tree")
rpart.plot(prunedTree, fallen.leaves = TRUE, main = "pruned Classification tree")
```
### 3.3.5 Predict tree model and Pruned tree model and setup confusion Matrix.

```{r}
predictions <- predict(Classification_tree, test, type = "class")

confMat_tree <- confusionMatrix(predictions, test$Outcome)

pruned_predictions <- predict(prunedTree, test, type = "class")

confMat_prune <- confusionMatrix(pruned_predictions, test$Outcome)
```
## 3.4 Random forest (`randomForest`)

### 3.4.1 Train random forest. plot important variables
```{r, results="hide"}
set.seed(202)

rf_model <- randomForest(Outcome ~ ., data = train, importance = TRUE)

num_vars <- ncol(train) - 1  #last column is the outcome

rf_tune_model <- tuneRF(
  x = train[, -which(names(train) == "Outcome")],    # Predictor variables
  y = train$Outcome,                                 # Response variable
  mtryStart = sqrt(num_vars),                        # Starting mtry
  ntreeTry = 500,                                    # Number of trees same as defualt
  stepFactor = 1,                                    # Factor to increase mtry
  improve = 0.1,                                     # Minimum improvement for tuning
  trace = FALSE                                      # Print progress
)

best_mtry <- rf_tune_model[which.min(rf_tune_model[, "OOBError"]), "mtry"]

final_rf_model <- randomForest(Outcome ~ ., data = train, mtry = best_mtry, importance = TRUE)
```
### 3.4.2 Predict Random forrest and Random forrest Tuned and setup confusion Matrix.

```{r}
predictions <- predict(rf_model, newdata = test)
confMat_RF <- confusionMatrix(predictions, test$Outcome)

predictions_tuned <- predict(final_rf_model, newdata = test)
confMat_RF_tuned <- confusionMatrix(predictions_tuned, test$Outcome)
```

## 3.5. Linear/quadratic discriminant analysis
LDA is preferable for simpler models with equal variances; QDA is better for more complex models with varying variances.

If LDA and QDA yield the same confusion matrices, consider choosing LDA for its interpretability and efficiency.

From 2.1 Response plot it suggest that we deal with two classes with equal variances hence a LDA is preferable.

But for the sake of it lets do both.

### 3.5.1 Linear discriminant analysis
```{r, results="hide"}
set.seed(202)
lda_model <- lda(Outcome ~ ., data = train)
```

### 3.5.2 Quadratic discriminant analysis

```{r, results="hide"}
set.seed(202)
qda_model <- qda(Outcome ~ ., data = train)
```
### 3.5.3 Predict LDA and QDA and setup confusion Matrix. 

```{r}
lda_pred <- predict(lda_model, test)
lda_classes <- lda_pred$class
confMat_lda <- confusionMatrix(lda_classes, test$Outcome)

qda_pred <- predict(qda_model, test)
qda_classes <- qda_pred$class
confMat_qda <- confusionMatrix(qda_classes, test$Outcome)
```
# Evaluation of all classifications models by using confusion matrices

```{r}
# Define a list of model names and their respective confusion matrices
models <- list(
  "Log" = confMat_log,
  "Log Refined" = confMat_log_refine,
  "Log Stepwise" = confMat_log_step,
  "Tree" = confMat_tree,
  "Tree Pruned" = confMat_prune,
  "Random Forest" = confMat_RF,
  "Random Forest Tuned" = confMat_RF_tuned,
  "LDA" = confMat_lda,
  "QDA" = confMat_qda
)

# Loop through each model and print the transposed confusion matrix and accuracy
for (model_name in names(models)) {
  conf_matrix <- models[[model_name]]
  
  cat("\n", model_name, "\n")  # Print model name
  
  # Transpose the confusion matrix
  transposed_conf_matrix <- t(conf_matrix$table)
  
  # Print transposed confusion matrix with labels for Prediction and Reference
  print(transposed_conf_matrix)  # Print the transposed confusion matrix
  
  # Print accuracy
  cat("Accuracy:", conf_matrix$overall['Accuracy'], "\n")
  cat("Precision:", conf_matrix$byClass['Pos Pred Value'], "\n")
  
  cat("\n-------------------------------------------------\n")  # Separate models
}
```


# Project dependencies

```{r}
sessionInfo()
```