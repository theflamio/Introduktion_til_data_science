---
title: "Data Science Exam: Part 2"
author: "Flemming Christensen"
date: "2024-10-6"
format:
  html:
    embed-resources: true
---

# Libraries
```{r, results="hide"}
library(doBy)
library(ggplot2)
library(reshape2)
library(rpart)
library(randomForest)
library(readr)
library(dplyr)
library(tidyr)
```


# Part 1: Regression

# Data
This part uses the `personality` dataset from the R package `doBy` with recordings of 32 variables describing personality characteristics for 240 people. (see help file for `?personality`)

We focus on the variable `easygon` as the response variable. We split the data in training and test data as follows:

```{r}
dat <- doBy::personality
set.seed(101) # for reproducibility
i_train <- sample(nrow(dat), .5*nrow(dat))
train <- dat[i_train,]
test <- dat[-i_train,]
```


# Exercise 1
Use `response_plot()` from the **development version of** `doBy` to visualize the relation between the response and the other variables in the data.

```{r, fig.width=10, fig.height=20}
response_plot(dat, easygon ~ ., geoms = c(geom_jitter(alpha = .6, width = .4, height = .4)), ncol = 5)
```


# Exercise 2
Specify a number of prediction models â€“ at least one of each of the following:

1. linear with stepwise selection (`lm`/ `step`)
2. regression tree (`rpart`)
3. random forest (`randomForest`)

## General observations
predictors that seems to be significant predictors from the response plot are following.

Very strong: relaxed - laidbck.
strong: coopera - approvn.

Lets see if that holds up later on.

## 1. linear with stepwise selection (`lm`/ `step`)

```{r, results="hide"}
# Fit a linear model
model <- lm(easygon ~ ., data = train)

# Perform stepwise selection based on AIC
stepwise_model <- step(model, direction = "both")

summary(stepwise_model)
```

## 2. regression tree (`rpart`)

```{r, fig.width=7, fig.height=7}
# Fit a regression tree model
tree_model <- rpart(easygon ~ ., data = train, method = "anova")

# Visualize the tree
plot(tree_model)
text(tree_model)
```

## 3. random forest (`randomForest`)

```{r, fig.width=7, fig.height=7}
set.seed(1024)

# Fit a random forest model
rf_model <- randomForest(easygon ~ ., data = train, importance = TRUE)

# Print model details
print(rf_model)

# Visualizes how the model's error rate changes with the number of trees.
plot(rf_model, main = "Random Forest Error Rates")

# Plots the importance of each variable, helping identify which predictors are most influential. "Only for random forrest"
varImpPlot(rf_model)
```

# Exercise 3
For **one of** the linear models show the estimated coefficients and comment **briefly** on the output (for example, comment on the sign of the estimates).

```{r}
summary(stepwise_model)
```


# Exercise 4
Compare the performance of the fitted models by predicting the values of `easygon` for the test data and show a plot comparing the predictions to the observations from the test data. Also make a table of the RMSE for each model.

```{r}
# Predict on test data
pred_step <- predict(model, newdata = test)
pred_rpart <- predict(tree_model, newdata = test)
pred_rf <- predict(rf_model, newdata = test)

# Extract actual values from the test set
actual <- test$easygon

# Function to calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Calculate RMSE for each model
rmse_step <- rmse(actual, pred_step)
rmse_rpart <- rmse(actual, pred_rpart)
rmse_rf <- rmse(actual, pred_rf)

# Create a table to compare RMSE
rmse_table <- data.frame(
  Model = c("Stepwise Linear", "Regression Tree", "Random Forest"),
  RMSE = c(rmse_step, rmse_rpart, rmse_rf)
)

# Print RMSE table
print(rmse_table)
```

```{r}
# Combine the actual and predicted values into a single data frame for plotting
plot_data <- data.frame(
  Observed = actual,
  Stepwise = pred_step,
  RegressionTree = pred_rpart,
  RandomForest = pred_rf
)

# Melt the data frame to long format for ggplot
plot_data_melt <- melt(plot_data, id.vars = "Observed", 
                       variable.name = "Model", value.name = "Predicted")

# Create the plot
ggplot(plot_data_melt, aes(x = Observed, y = Predicted, color = Model)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = "Observed vs Predicted Values of easygon",
       x = "Observed easygon", y = "Predicted easygon") +
  facet_wrap(~Model)
```


# Part 2: Classification
The dataset `diabetes.csv` is about risk factors for diabetes and contains 768 observations of 9 variables. The response variable is `Outcome` which is binary. The goal is to predict `Outcome` using the other variables. A description of the dataset can be found at https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data

# Exercise 1
Read in and clean the data by detecting unexpected values of some variables, replace the values with `NA` and finally make a data set with only the complete cases.

```{r, results="hide"}
data_dir <- here::here("diabetes.csv") #set the directory to were the project and dataset is used

dia_raw_dat = suppressMessages(read_csv(data_dir)) # Read csv file

head(dia_raw_dat) #check for correct data type
```
```{r}
summary(dia_raw_dat) #quick and dirty look at the raw data
```


# Diabetes information and dataset observations

1. Pregnancies: Number of times pregnant "Obs missing genders male/female"

2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test

3. BloodPressure: Diastolic blood pressure (mm Hg)

4. SkinThickness: Triceps skin fold thickness (mm)

5. Insulin: 2-Hour serum insulin (mu U/ml)

6. BMI: Body mass index (weight in kg/(height in m)^2)

7. DiabetesPedigreeFunction aka genetic risk factor: DPF of 0.5 = moderate risk and DPF of 0.9 high genetic risk factor.

8. Age: Age (years)

9. Outcome: Class variable (0 or 1)

zero value in any of the variable beside Pregnancy and Outcome is unrealistic

Seems like the data set have used 0 and 0.0 to describe missing values. Needs to be changed to NA

```{r}
dia_dat_with_NA <- dia_raw_dat |>
  mutate(	
    Glucose = ifelse(Glucose == 0, NA, Glucose),
    BloodPressure = ifelse(BloodPressure == 0, NA, BloodPressure),
    SkinThickness = ifelse(SkinThickness == 0, NA, SkinThickness),
    Insulin = ifelse(Insulin == 0, NA, Insulin),
    DiabetesPedigreeFunction = ifelse(DiabetesPedigreeFunction == 0, NA, DiabetesPedigreeFunction)
  )

dia_clean_dat <- na.omit(dia_dat_with_NA)

summary(dia_clean_dat) #quick and dirty look at the clean data
```


# Exercise 2
Plot the response against each of the explanatory variables/features and add a smooth line as a very rough indication of how the probability of diabetes depends on the individual feature (note the response has to be numeric for the smooth line to work).

```{r, fig.width=7, fig.height=15}

# Convert data to long format to use facetting
dia_long <- dia_clean_dat |>
  pivot_longer(cols = c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age"),
               names_to = "Feature",
               values_to = "Value")

dia_long

ggplot(dia_long, aes(x = Value, y = Outcome)) +
  geom_point(position = position_jitter(width = 0, height = 0.1),alpha = 0.5) +  # Scatterplot
  geom_smooth(method = "loess", color = "blue", se = TRUE) +  # Smooth line with confidence interval
  facet_wrap(~ Feature, scales = "free_x") +  # Facet by Feature, adjust scales
  labs(
    title = "Outcome vs Features",
    x = "Feature Value",
    y = "Probability of Diabetes aka Outcome"
  )
```


# Exercise 3
Ensure Outcome is a factor and split the data in a training and test set as follows:

```{r}
set.seed(202) # for reproducibility
i_train <- sample(nrow(dat), .5*nrow(dat))
train <- dat[i_train,]
test <- dat[-i_train,]
```

Specify a number of classification models and report the corresponding confusion matrix for the test data. You must include at least one of each of the following:

1. logistic regression with stepwise selection (`glm` / `step`)
2. regression tree (`rpart`)
3. random forest (`randomForest`)
4. linear/quadratic discriminant analysis

## 1. logistic regression with stepwise selection (`glm` / `step`)

```{r}

```

## 2. regression tree (`rpart`)

```{r}

```

## 3. random forest (`randomForest`)

```{r}

```

## 4. linear/quadratic discriminant analysis

```{r}

```

## Project dependencies

```{r}
sessionInfo()
```